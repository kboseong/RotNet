# RotNet paper review
[](https://arxiv.org/pdf/1803.07728.pdf)

## Abstract

ConvNet which learn image features by recognizing the 2d rotation that is applied to the image can learn the semantic feature of image data well.

## Introduction

### Self-supervised Learning

Even though ConvNets trained in supervised-manner are working well in many areas, supervised feature learning has a significant limitation of requiring intensive manual labeling effort. 

Since there are vast amount of image data that cannot be labeled, interest in training ConvNets with unsupervised-manner is growing, especially in self-supervised learning.

Self-supervised learning defines an annotation free pretext task, using only the visual information present on the images or videos, in order to provide a surrogate supervision signal for feature learning. Examples for the self-supervised tasks are,

- To colorize gray scale images
- To predict the relative position of image patches
- To predict self-motion of a moving vehicle between two consecutive frames

The background rationale of these tasks is that solving them will force the ConvNet to learn semantic image features that can be useful for the other vision tasks, such as object recognition, object detection, and semantic segmentation.

### Task Description

Generally, the task proposed by this paper is to recognize the geometric transformation applied to the image. The paper defines the geometric transformation as the image rotations by 0, 90, 180, and 270 degrees.

The paper argues that recognizing the rotation transformation requires the model to understand the concept of the object such as location, type, and pose.

## Methodology

### Overview

The loss is defined by the average of Negative Log Likelihood(Cross Entropy Loss) of probability of properly predicting the each geometric transformation.

### Choosing Geometric Transformations: Image Rotations

The paper chose image rotation as the geometric transformation because,

- **It forces the model to learn semantic features.** To successfully predict the rotation of an image, the model must necessarily learn to localize salient objects in the image, recognize their orientation and object type, and then relate the object orientation with the dominant orientation that each type of object tends to be depicted within the available images. The evidence of this argument is that the attention maps of the model trained by the task, and the ordinary supervised classification task was similar.
- **It does not make a low-level visual artifacts.** Other geometric transformations such as scaling or changing aspect ratio of image can leave an easily detectable image artifacts.
- **Object orientation is well-defined.** There is usually no ambiguity of what is the rotation transformation.
- **It is easy to implement**.

## Experimental Results

### CIFAR Expermiments

**Implementation Details**

- Implemented RotNet with Network-In-Network (NIN) architectures.
- Training by feeding all the four rotated copies of an image simultaneously improved the model significantly.

**Evaluation of the Learned Feature Hierarchies**

- Trained RotNet with 3, 4, and 5 convolutional blocks (each block has 3 convolutional layers), then learned classifiers on top of the feature maps generated by each convolutional blocks.
- Feature map generated by the second convolutional block achieved the highest accuracy in all cases.
    - After the second block, they started to become more and more specific on the rotation prediction task.
- Increasing the total depth of the RotNet models increased the object recognition performance by the feature maps generated by earlier layers.
    - Increasing the depth of the model increases the complexity of its head, which allows the features of earlier layers to be less specific to the rotation prediction task.

**Exploring the quality of the learned features w.r.t. the number of recognized rotations**

- 4 discrete rotations achieved the best performance.
    - The 2 orientations case offers too few class for recognition.
    - The 8 orientations case are not distinguishable enough and made visual artifacts while rotating the image.
- 90 and 270 degree case got worse performance than 0 and 180 degree case.
    - 90 and 270 degree case does not see the original image which is used in the object recognition task.

**Comparison against supervised and other unsupervised methods**

- Used the feature map generated by the second convolutional block of a RotNet model with 4 convolutional blocks in total.
- Used two different classifiers:
    - 3 fully connected layers
    - 3 convolutional layers and a linear prediction layer
- Achieved SOTA results in CIFAR-10 with unsupervised-manner.
    - However, the model architectures are different from the other unsupervised models.
- Accuracy gap between the fully supervised NIN model was only 1.64%.

**Correlation between object classification task and rotation prediction task**

- As the ability of the RotNet features for solving the rotation prediction task improves, their ability to help solving the object recognition task improves as well.
- The object recognition accuracy converges fast w.r.t. the number of training epochs.

**Semi-supervised setting**

- First train the RotNet with the whole dataset, and then train classifier with the feature map generated by the RotNet with 20, 100, 400, 1000, and 5000 images for each class.
- And compare the performance with a supervised model trained with the same amount of data.
- Semi-supervised model exceeded the supervised model when the number of image per class drops below 1000.
- The performance gap became wider as the dataset gets smaller.

### Evaluation of Self-Supervised Features Trained in ImageNet

**Implementation Details**

- Implemented with AlexNet architecture.
- Trained in total for 30 epochs.

**ImageNet classification task** 

- The approach of the paper surpassed all the other methods by a significant margin.

**Transfer learning evaluation on PASCAL VOC**

- Fine-tuned for image classification, detection, and segmentation tasks.
- Outperformed by significant margin all the competing unsupervised methods in all tested tasks.

**Places Classification Task**

- Logistic regression.